{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from math import log2\n",
    "\n",
    "# Path to your QA folder\n",
    "qa_dir = \"../../OmniMedVQA/QA_information\"\n",
    "images_dir = \"../../OmniMedVQA/Images\"\n",
    "\n",
    "open_access_dir = os.path.join(qa_dir, \"Open-access\")\n",
    "restricted_access_dir = os.path.join(qa_dir, \"Restricted-access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect answers grouped by dataset\n",
    "dataset_answers = defaultdict(list)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Iterate through Open- and Restricted-access JSONs\n",
    "for folder in [open_access_dir, restricted_access_dir]:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                data = load_json(file_path)\n",
    "                for item in data:\n",
    "                    dataset_name = item.get(\"dataset\", \"Unknown\")\n",
    "                    answer = item.get(\"gt_answer\", None)\n",
    "                    if answer:\n",
    "                        dataset_answers[dataset_name].append(answer)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_plots = 3  # how many datasets to plot\n",
    "for i, (dataset_name, answers) in enumerate(dataset_answers.items()):\n",
    "    if i >= max_plots:  # stop after X plots\n",
    "        break\n",
    "    \n",
    "    counts = Counter(answers)\n",
    "    labels, values = zip(*counts.most_common())\n",
    "    \n",
    "    plt.figure(figsize=(10, max(4, len(labels) * 0.4)))\n",
    "    plt.barh(labels, values)\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Answer\")\n",
    "    plt.title(f\"Answer Distribution for {dataset_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Answer Distribution\n",
    "\n",
    "The charts below show the distribution of answer choices for a few example datasets.  \n",
    "For instance, the **Adam Challenge** (and several others with similar patterns) exhibit a strong skew toward one particular answer choice.  \n",
    "This indicates a noticeable **answer bias**, where one option dominates the dataset.  \n",
    "As a result, it becomes more difficult to draw balanced conclusions since there are fewer examples for the less frequent answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "\n",
    "def entropy(probs):\n",
    "    return -sum(p * log2(p) for p in probs if p > 0)\n",
    "\n",
    "uniformity_scores = {}\n",
    "\n",
    "for dataset_name, answers in dataset_answers.items():\n",
    "    counts = Counter(answers)\n",
    "    total = sum(counts.values())\n",
    "    probs = [c/total for c in counts.values()]\n",
    "    \n",
    "    if len(counts) == 1:\n",
    "        # If only one unique answer, define uniformity as 100%\n",
    "        uniformity = 100.0\n",
    "    else:\n",
    "        # entropy of actual distribution\n",
    "        H_actual = entropy(probs)\n",
    "        # entropy of uniform distribution (max possible)\n",
    "        H_uniform = log2(len(counts))\n",
    "        uniformity = (H_actual / H_uniform) * 100\n",
    "    \n",
    "    uniformity_scores[dataset_name] = uniformity\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(uniformity_scores.keys(), uniformity_scores.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Uniformity %\")\n",
    "plt.title(\"Answer Distribution Uniformity Across Datasets\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Answer Distribution Uniformity\n",
    "\n",
    "This plot highlights which datasets have the lowest answer uniformity, meaning their results are heavily skewed toward one option.  \n",
    "For example, **Refuge2** and **AIROGS** show some of the strongest imbalances, with one answer choice appearing far more often than the others.  \n",
    "Such skew makes it harder for models to learn balanced decision-making across all possible answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many questions per dataset\n",
    "dataset_sizes = {dataset_name: len(answers) for dataset_name, answers in dataset_answers.items()}\n",
    "\n",
    "cap_value = 10000\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "bars = plt.bar(dataset_sizes.keys(), \n",
    "               [min(v, cap_value) for v in dataset_sizes.values()])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Number of Questions (capped at 10k)\")\n",
    "plt.title(\"Number of Questions per Dataset\")\n",
    "\n",
    "# Annotate bars that exceed the cap\n",
    "for bar, (dataset_name, size) in zip(bars, dataset_sizes.items()):\n",
    "    if size > cap_value:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                 cap_value, \n",
    "                 f\"{size}\", \n",
    "                 ha='center', va='bottom', fontsize=9, rotation=90, color=\"red\")\n",
    "\n",
    "plt.ylim(0, cap_value*1.1)  # give some space above cap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Number of Questions per Dataset\n",
    "\n",
    "This plot shows how the number of questions varies across datasets.  \n",
    "For instance, **RadImageNet** contains nearly 57k questions, while the next largest dataset has only around 10k.  \n",
    "This imbalance means that some datasets are far more represented than others, which is important to keep in mind when drawing conclusions from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_value = 5000\n",
    "\n",
    "# Allowed image extensions\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif')\n",
    "\n",
    "# Dictionary to store counts\n",
    "image_counts = {}\n",
    "\n",
    "# Walk through each subdirectory in Images\n",
    "for subdir in os.listdir(images_dir):\n",
    "    subdir_path = os.path.join(images_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        count = 0\n",
    "        # Recursively walk through the subdirectory\n",
    "        for root, _, files in os.walk(subdir_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(image_extensions):\n",
    "                    count += 1\n",
    "        image_counts[subdir] = count\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14,6))\n",
    "bars = plt.bar(image_counts.keys(), [min(v, cap_value) for v in image_counts.values()])\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Number of Images (capped at 10k)\")\n",
    "plt.title(\"Number of Images per Class\")\n",
    "\n",
    "# Annotate bars that exceed the cap\n",
    "for bar, (subdir, count) in zip(bars, image_counts.items()):\n",
    "    if count > cap_value:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2,\n",
    "                 cap_value,\n",
    "                 f\"{count}\",\n",
    "                 ha='center', va='bottom', fontsize=9, rotation=90, color=\"red\")\n",
    "\n",
    "plt.ylim(0, cap_value*1.1)  # some space above the cap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Number of Images per Dataset\n",
    "\n",
    "The distribution of images follows a similar pattern to the question counts.  \n",
    "For example, **RadImageNet** contains nearly 56k images, while the next largest dataset has only about 3k.  \n",
    "This imbalance again highlights how certain datasets dominate the collection, which can influence model training and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
