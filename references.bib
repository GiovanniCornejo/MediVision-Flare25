@misc{Gapp2024_LLaMA2Med,
  title     = {Multimodal Medical Disease Classification with LLaMA II},
  author    = {Christian Gapp and Elias Tappeiner and Martin Welk and Rainer Schubert},
  year      = {2024},
  eprint    = {2412.01306},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  doi       = {10.48550/arXiv.2412.01306},
  url       = {https://arxiv.org/abs/2412.01306},
  note      = {Chest X-ray + clinical report multimodal disease classification using LLaMA II (7B) with vision-language fusion strategies (early/late/mixed) and LoRA fine-tuning. Achieves state-of-the-art mean AUC (0.971) on OpenI dataset. Relevant for multimodal classification task.}
}


@InProceedings{Moor2023_MedFlamingo,
  title     = {Med-Flamingo: a Multimodal Medical Few-shot Learner},
  author    = {Michael Moor and Qian Huang and Shirley Wu and Michihiro Yasunaga and Yash Dalmia and Jure Leskovec and Cyril Zakka and Eduardo Pontes Reis and Pranav Rajpurkar},
  booktitle = {Proceedings of the 3rd Machine Learning for Health Symposium},
  pages     = {353--367},
  year      = {2023},
  editor    = {Stefan Hegselmann and Antonio Parziale and Divya Shanmugam and Shengpu Tang and Mercy Nyamewaa Asiedu and Serina Chang and Tom Hartvigsen and Harvineet Singh},
  volume    = {225},
  series    = {Proceedings of Machine Learning Research},
  month     = {10 Dec},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v225/moor23a/moor23a.pdf},
  url       = {https://proceedings.mlr.press/v225/moor23a.html},
  note      = {Introduces Med-Flamingo, a few-shot multimodal medical VQA model built on Flamingo. Uses interleaved text-image pretraining (MTB, PMC-OA), supports in-context learning, and excels at generating clinically relevant open-ended answers. Highlights dataset creation, human evaluation, and limitations such as hallucinations and underrepresented pathology. Relevant for multimodal fusion, few-shot learning, and complex reasoning in medical AI.}
}

@INPROCEEDINGS{Van2024_LargeVLMsMed,
  author    = {Minhâ€“Hao Van and Prateek Verma and Xintao Wu},
  title     = {On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study},
  booktitle = {2024 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)},
  pages     = {172--176},
  year      = {2024},
  doi       = {10.1109/CHASE60773.2024.00029},
  note      = {Evaluates pretrained visual-language models (BiomedCLIP, OpenCLIP, OpenFlamingo, LLaVA, ChatGPT-4) on medical imaging tasks (brain MRI, blood microscopy, COVID X-rays). Highlights zero-shot and few-shot performance, prompt engineering strategies, and limitations compared to CNN-based models. Relevant for multimodal VLM evaluation and prompt design in medical imaging.}
}
