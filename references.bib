@misc{Gapp2024_LLaMA2Med,
  title     = {Multimodal Medical Disease Classification with LLaMA II},
  author    = {Christian Gapp and Elias Tappeiner and Martin Welk and Rainer Schubert},
  year      = {2024},
  eprint    = {2412.01306},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  doi       = {10.48550/arXiv.2412.01306},
  url       = {https://arxiv.org/abs/2412.01306},
  note      = {Chest X-ray + clinical report multimodal disease classification using LLaMA II (7B) with vision-language fusion strategies (early/late/mixed) and LoRA fine-tuning. Achieves state-of-the-art mean AUC (0.971) on OpenI dataset. Relevant for multimodal classification task.}
}


@InProceedings{Moor2023_MedFlamingo,
  title     = {Med-Flamingo: a Multimodal Medical Few-shot Learner},
  author    = {Michael Moor and Qian Huang and Shirley Wu and Michihiro Yasunaga and Yash Dalmia and Jure Leskovec and Cyril Zakka and Eduardo Pontes Reis and Pranav Rajpurkar},
  booktitle = {Proceedings of the 3rd Machine Learning for Health Symposium},
  pages     = {353--367},
  year      = {2023},
  editor    = {Stefan Hegselmann and Antonio Parziale and Divya Shanmugam and Shengpu Tang and Mercy Nyamewaa Asiedu and Serina Chang and Tom Hartvigsen and Harvineet Singh},
  volume    = {225},
  series    = {Proceedings of Machine Learning Research},
  month     = {10 Dec},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v225/moor23a/moor23a.pdf},
  url       = {https://proceedings.mlr.press/v225/moor23a.html},
  note      = {Introduces Med-Flamingo, a few-shot multimodal medical VQA model built on Flamingo. Uses interleaved text-image pretraining (MTB, PMC-OA), supports in-context learning, and excels at generating clinically relevant open-ended answers. Highlights dataset creation, human evaluation, and limitations such as hallucinations and underrepresented pathology. Relevant for multimodal fusion, few-shot learning, and complex reasoning in medical AI.}
}

@INPROCEEDINGS{Van2024_LargeVLMsMed,
  author    = {Minhâ€“Hao Van and Prateek Verma and Xintao Wu},
  title     = {On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study},
  booktitle = {2024 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)},
  pages     = {172--176},
  year      = {2024},
  doi       = {10.1109/CHASE60773.2024.00029},
  note      = {Evaluates pretrained visual-language models (BiomedCLIP, OpenCLIP, OpenFlamingo, LLaVA, ChatGPT-4) on medical imaging tasks (brain MRI, blood microscopy, COVID X-rays). Highlights zero-shot and few-shot performance, prompt engineering strategies, and limitations compared to CNN-based models. Relevant for multimodal VLM evaluation and prompt design in medical imaging.}
}

@misc{Xia2025_MMedRAG,
  title     = {MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models},
  author    = {Peng Xia and Kangyu Zhu and Haoran Li and Tianze Wang and Weijia Shi and Sheng Wang and Linjun Zhang and James Zou and Huaxiu Yao},
  year      = {2025},
  eprint    = {2410.13085},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url       = {https://arxiv.org/abs/2410.13085},
  note      = {Proposes MMed-RAG, a multimodal retrieval-augmented generation system for medical vision-language models (Med-LVLMs). Combines domain-aware retrieval, adaptive context selection, and preference fine-tuning to improve factuality, cross-modality alignment, and overall performance. Achieves significant gains on medical VQA and report generation tasks across radiology, pathology, and ophthalmology datasets. Relevant for enhancing factuality and mitigating misalignment in multimodal medical LLMs.}
}
