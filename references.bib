@misc{Gapp2024_LLaMA2Med,
  title     = {Multimodal Medical Disease Classification with LLaMA II},
  author    = {Christian Gapp and Elias Tappeiner and Martin Welk and Rainer Schubert},
  year      = {2024},
  eprint    = {2412.01306},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  doi       = {10.48550/arXiv.2412.01306},
  url       = {https://arxiv.org/abs/2412.01306},
  note      = {Chest X-ray + report classification with LLaMA II and vision-language fusion (early/late/mixed) + LoRA fine-tuning. Achieves SOTA AUC (0.971) on OpenI. Relevant for multimodal classification. Discusses Early, Late, Mixed fusion pipelines}
}

@InProceedings{Moor2023_MedFlamingo,
  title     = {Med-Flamingo: a Multimodal Medical Few-shot Learner},
  author    = {Michael Moor and Qian Huang and Shirley Wu and Michihiro Yasunaga and Yash Dalmia and Jure Leskovec and Cyril Zakka and Eduardo Pontes Reis and Pranav Rajpurkar},
  booktitle = {Proceedings of the 3rd Machine Learning for Health Symposium},
  pages     = {353--367},
  year      = {2023},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v225/moor23a.html},
  note      = {Introduces Med-Flamingo, a few-shot multimodal medical VQA model with interleaved text-image pretraining. Strong in-context learning, human evaluation, but limited by hallucinations and missing pathologies.}
}

@INPROCEEDINGS{Van2024_LargeVLMsMed,
  author    = {Minh–Hao Van and Prateek Verma and Xintao Wu},
  title     = {On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study},
  booktitle = {2024 IEEE/ACM CHASE},
  pages     = {172--176},
  year      = {2024},
  doi       = {10.1109/CHASE60773.2024.00029},
  note      = {Benchmarks VLMs (BiomedCLIP, OpenCLIP, Flamingo, LLaVA, GPT-4) on brain MRI, microscopy, COVID X-rays. Compares zero-/few-shot, prompts, vs CNN baselines. Relevant for VLM eval.}
}

@misc{Xia2025_MMedRAG,
  title     = {MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models},
  author    = {Peng Xia and Kangyu Zhu and Haoran Li and Tianze Wang and Weijia Shi and Sheng Wang and Linjun Zhang and James Zou and Huaxiu Yao},
  year      = {2025},
  eprint    = {2410.13085},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url       = {https://arxiv.org/abs/2410.13085},
  note      = {Proposes MMed-RAG with domain-aware retrieval and context selection. Improves factuality and cross-modality alignment on medical VQA/report generation.}
}

@misc{Byra2023_FewShotVLM,
  title     = {Few-shot medical image classification with simple shape and texture text descriptors using vision-language models}, 
  author    = {Michal Byra and Muhammad Febrian Rachmadi and Henrik Skibbe},
  year      = {2023},
  eprint    = {2308.04005},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  url       = {https://arxiv.org/abs/2308.04005},
  note      = {Uses GPT-4–generated shape/texture descriptors with CLIP for few-shot classification (X-ray, ultrasound). Improves n-shot performance where data is limited.}
}

@misc{Wang2022_MedCLIP,
  title     = {MedCLIP: Contrastive Learning from Unpaired Medical Images and Text},
  author    = {Zifeng Wang and Zhenbang Wu and Dinesh Agarwal and Jimeng Sun},
  year      = {2022},
  eprint    = {2210.10163},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url       = {https://arxiv.org/abs/2210.10163},
  note      = {Contrastive pretraining with UMLS/MetaMap to align image, text, and paired data. Strong zero-shot classification/retrieval on chest X-rays, beating ConVIRT/GLoRIA with less data.}
}

@inbook{Lei2023_CLIP_Lung,
  title     = {CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction},
  booktitle = {MICCAI 2023},
  author    = {Lei, Yiming and Li, Zilong and Shen, Yan and Zhang, Junping and Shan, Hongming},
  year      = {2023},
  pages     = {403--412},
  note      = {Lung nodule malignancy prediction via CT + textual annotations. Uses channel-wise conditional prompt and knowledge-guided contrastive learning. SOTA on LIDC-IDRI.}
}

@misc{Li2023_LLaVA_Med,
  title     = {LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day},
  author    = {Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  year      = {2023},
  eprint    = {2306.00890},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url       = {https://arxiv.org/abs/2306.00890},
  note      = {Adapts LLaVA to biomedical domain using PMC-15M. Two-stage curriculum (concept alignment + instruction tuning). Strong biomedical VQA with low cost.}
}

@article{Haq2025_Advancements_MMML_Radiology,
  title={Advancements in Medical Radiology Through Multimodal Machine Learning: A Comprehensive Overview},
  author={Haq, Imran Ullah and et al.},
  journal={Bioengineering (Basel)},
  volume={12},
  number={5},
  pages={477},
  year={2025},
  doi={10.3390/bioengineering12050477},
  note={Survey of multimodal ML in radiology. Covers fusion (early/late/joint), representation learning, cross-modal retrieval. Applications: classification, VQA. Notes future directions incl. time-series, beyond bimodal.}
}

@article{Seki2025_ZeroShotECG,
  title={Assessing zero-shot VQA in multimodal LLMs for 12-lead ECG interpretation},
  author={Seki, Takumi and et al.},
  journal={Frontiers in Cardiovascular Medicine},
  volume={12},
  pages={1458289},
  year={2025},
  doi={10.3389/fcvm.2025.1458289},
  note={Tests ViLT, Gemini Pro Vision, ChatGPT on ECG VQA. Models biased toward “normal”; ChatGPT best but still hallucinates. Highlights evaluation and structured data needs.}
}

@article{Sharma2021_MedFuseNet,
  title={MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain},
  author={Sharma, Divya and Purushotham, Sanjay and Reddy, Chandan K.},
  journal={Scientific Reports},
  volume={11},
  number={1},
  pages={19826},
  year={2021},
  doi={10.1038/s41598-021-98390-1},
  note={Attention-based multimodal fusion for medical VQA. Achieves SOTA with interpretable attention visualizations.}
}

@article{Lu2024CoDVQA,
  title={Collaborative Modality Fusion for Mitigating Language Bias in VQA},
  author={Lu, Qi and Chen, Shuang and Zhu, Xiaowei},
  journal={Journal of Imaging},
  volume={10},
  number={3},
  pages={56},
  year={2024},
  doi={10.3390/jimaging10030056},
  note={CoD-VQA: collaborative modality fusion reducing language/vision bias in VQA by allowing rich modalities to assist deprived ones.}
}

@article{Abdullakutty2024_MultiModalHistopathology,
  title = {Histopathology in focus: a review on explainable multi-modal approaches for breast cancer diagnosis},
  author = {Abdullakutty, Fathima and et al.},
  journal = {Frontiers in Medicine},
  volume = {11},
  pages = {1450103},
  year = {2024},
  doi = {10.3389/fmed.2024.1450103},
  note = {Review of multi-modal breast cancer histopathology methods. Compares unimodal vs multimodal (histopath, clinical, genomic). Discusses fusion strategies, imbalance issues, clinical applicability.}
}

@article{Sun2023_MultimodalReview,
  title = {A scoping review on multimodal deep learning in biomedical images and texts},
  author = {Sun, Zhi and et al.},
  journal = {Journal of Biomedical Informatics},
  volume = {146},
  pages = {104482},
  year = {2023},
  doi = {10.1016/j.jbi.2023.104482},
  note = {Review of multimodal deep learning in biomedicine (images + text). Covers report generation, VQA, retrieval, CAD. Notes challenges: imbalance, data scarcity, limited modalities, fairness/interpretability. Suggests dataset creation, knowledge integration, stronger evals.}
}
